{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5b20eb",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ELTE-DSED/Intro-Data-Security/blob/main/module_02_input_manipulation/Lab2_Evasion_Attacks.ipynb)\n",
    "\n",
    "# **Lab 2: Evasion Attacks (Adversarial Examples)**\n",
    "\n",
    "**Course:** Introduction to Data Security Pr.  \n",
    "**Module 2:** Input Data Manipulation  \n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, we explore how adversaries can manipulate input data to deceive machine learning models. Adversarial examples are carefully crafted inputs that appear normal to humans but cause models to make incorrect predictions with high confidence. These attacks highlight vulnerabilities in even state-of-the-art neural networks and motivate the need for robust defenses.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/openai_panda.png\" alt=\"Adversarial Example: Panda to Gibbon\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccadae0",
   "metadata": {},
   "source": [
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Generate adversarial examples using FGSM and PGD\n",
    "- Measure model robustness under $\\ell_\\infty$ perturbations\n",
    "- Visualize perturbations and their impact on predictions\n",
    "- Compare clean vs. adversarial accuracy\n",
    "- Discuss the robustnessâ€“accuracy tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3d3e3",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "1. [Setup & Imports](#setup)  \n",
    "2. [Load Model & Dataset](#data)  \n",
    "3. [Baseline Evaluation](#baseline)  \n",
    "4. [FGSM Attack](#fgsm)  \n",
    "5. [PGD Attack ($\\ell_\\infty$ and $\\ell_2$)](#pgd)  \n",
    "6. [Targeted Evasion Attacks](#targeted)  \n",
    "7. [Adversarial Training (Defense)](#defense)  \n",
    "8. [Library Implementation (secml-torch)](#library)  \n",
    "9. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111b8bb",
   "metadata": {},
   "source": [
    "## **1. Setup & Imports** <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662a648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# If needed, install dependencies\n",
    "%pip install secml-torch -q\n",
    "\n",
    "# Importing all the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import test_untargeted_attack\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85042f4a",
   "metadata": {},
   "source": [
    "## **2. Load Model & Dataset** <a name=\"data\"></a>\n",
    "\n",
    "We will train MNIST model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4595c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized clamp range: [-0.42, 2.82]\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST test set\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Optional train set for fallback training\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define the MLP model from Lab 1\n",
    "class MNISTNet(torch.nn.Module):\n",
    "    \"\"\"Simple fully connected network for MNIST classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(784, 200)\n",
    "        self.fc2 = torch.nn.Linear(200, 200)\n",
    "        self.fc3 = torch.nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "model = MNISTNet().to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Normalized clamp bounds for MNIST\n",
    "mnist_mean = 0.1307\n",
    "mnist_std = 0.3081\n",
    "min_val = (0.0 - mnist_mean) / mnist_std\n",
    "max_val = (1.0 - mnist_mean) / mnist_std\n",
    "print(f\"Normalized clamp range: [{min_val:.2f}, {max_val:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d40cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean accuracy: 95.98%\n"
     ]
    }
   ],
   "source": [
    "from secmlt.models.pytorch.base_pytorch_nn import BasePytorchClassifier\n",
    "from secmlt.models.pytorch.base_pytorch_trainer import BasePyTorchTrainer\n",
    "from secmlt.metrics.classification import Accuracy\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the trainer for 1 epoch\n",
    "trainer = BasePyTorchTrainer(optimizer=optimizer, epochs=1)\n",
    "\n",
    "# Wrap the model and train\n",
    "secml_model = BasePytorchClassifier(model=model, trainer=trainer)\n",
    "secml_model.train(dataloader=train_loader)\n",
    "\n",
    "# Evaluate accuracy\n",
    "secml_accuracy = Accuracy()(secml_model, test_loader)\n",
    "print(f\"Clean accuracy: {secml_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bfff0",
   "metadata": {},
   "source": [
    "## **3. FGSM Attack** <a name=\"fgsm\"></a>\n",
    "### **3.1 Untargeted FGSM**\n",
    "\n",
    "The Fast Gradient Sign Method (FGSM) proposed by [Goodfellow et al.](https://arxiv.org/pdf/1412.6572.pdf)  crafts adversarial examples by taking a single step in the direction of the input gradient.\n",
    "\n",
    "The attack constructs adversarial examples as follows:\n",
    "\n",
    "$$x_\\text{adv} = x + \\epsilon\\cdot\\text{sign}(\\nabla_xJ(\\theta, x, y))$$\n",
    "\n",
    "where \n",
    "\n",
    "*   $x_\\text{adv}$ : Adversarial image.\n",
    "*   $x$ : Original input image.\n",
    "*   $y$ : Original input label.\n",
    "*   $\\epsilon$ : Multiplier to ensure the perturbations are small.\n",
    "*   $\\theta$ : Model parameters.\n",
    "*   $J$ : Loss.\n",
    "\n",
    "The current attack formulation is considered 'untargeted' because it only seeks to maximize loss rather than to trick the model into predicting a specific label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f6464",
   "metadata": {},
   "source": [
    "Try implementing the untargeted FGSM method for a batch of images yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untargeted_fgsm(x_batch, true_labels, network, normalize, eps=8/255., **kwargs):\n",
    "  \"\"\"Generates a batch of untargeted FGSM adversarial examples\n",
    "\n",
    "  Args:\n",
    "    x_batch (torch.Tensor): the batch of unnormalized input examples.\n",
    "    true_labels (torch.Tensor): the batch of true labels of the example.\n",
    "    network (nn.Module): the network to attack.\n",
    "    normalize (function): a function which normalizes a batch of images \n",
    "        according to standard MNIST normalization.\n",
    "    eps (float): the bound on the perturbations.\n",
    "  \"\"\"\n",
    "  loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "  x_batch = x_batch.clone().detach().requires_grad_(True)\n",
    "\n",
    "  #########################\n",
    "  # Enter Your Code Here! #\n",
    "  #########################\n",
    "  # (Hint: normalize x_batch, get logits, compute loss, \n",
    "  # backprop, compute perturbation, add it to original x_batch, \n",
    "  # and clamp to normalized bounds)\n",
    "\n",
    "  return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f259a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the method\n",
    "test_untargeted_attack(untargeted_FGSM, model, device, eps=8/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053de4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def untargeted_fgsm(model, x, y, epsilon=0.3):\n",
    "    x = x.clone().detach().to(device)\n",
    "    y = y.to(device)\n",
    "    x.requires_grad = True\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = nn.CrossEntropyLoss()(logits, y)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    perturbation = epsilon * x.grad.sign()\n",
    "    x_adv = x + perturbation\n",
    "    x_adv = torch.clamp(x_adv, min_val, max_val)\n",
    "    return x_adv.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a7caa",
   "metadata": {},
   "source": [
    "Instead of manually implementing the attack, we can use the `secmlt` library, which provides a unified API for various adversarial attacks. Here we demonstrate the FGSM attack using `secmlt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secmlt.adv.evasion.pgd import PGD\n",
    "from secmlt.metrics.classification import Accuracy\n",
    "from secmlt.adv.backends import Backends\n",
    "from secmlt.adv.evasion.perturbation_models import LpPerturbationModels\n",
    "\n",
    "# MNIST normalization constants\n",
    "mnist_mean = 0.1307\n",
    "mnist_std = 0.3081\n",
    "# Convert image bounds [0, 1] to normalized space\n",
    "min_val = (0.0 - mnist_mean) / mnist_std\n",
    "max_val = (1.0 - mnist_mean) / mnist_std\n",
    "\n",
    "# Test FGSM attack at multiple perturbation magnitudes\n",
    "epsilons = [0.0, 0.05, 0.1, 0.2]\n",
    "\n",
    "# Define the FGSM attack using secmlt\n",
    "results_secmlt = {}\n",
    "\n",
    "for eps in epsilons:\n",
    "    if eps == 0:\n",
    "        acc = Accuracy()(secml_model, test_loader)\n",
    "    else:\n",
    "        # Instantiate the attack as a 1-step PGD (FGSM) with native backend\n",
    "        # num_steps=2: first step computes and applies perturbation, second step evaluates\n",
    "        attack = PGD(\n",
    "            perturbation_model=LpPerturbationModels.LINF, \n",
    "            epsilon=eps, \n",
    "            num_steps=2,  # FGSM is single-step; num_steps=2 evaluates step 1's perturbation\n",
    "            step_size=eps, \n",
    "            random_start=False,\n",
    "            lb=min_val, \n",
    "            ub=max_val, \n",
    "            backend=Backends.NATIVE\n",
    "        )\n",
    "        \n",
    "        # Generate adversarial examples and evaluate accuracy\n",
    "        adv_loader = attack(secml_model, test_loader)\n",
    "        acc = Accuracy()(secml_model, adv_loader)\n",
    "    \n",
    "    results_secmlt[eps] = acc\n",
    "    print(f\"SecML-Torch FGSM epsilon={eps:.2f} -> accuracy: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f823b6",
   "metadata": {},
   "source": [
    "SecML-Torch's PGD with `num_steps=2` performs exactly 1 step at size `step_size`, effectively implementing FGSM. Setting `num_steps=2` (step 1 computes perturbation, step 2 is final) evaluates the best perturbation found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7e40c",
   "metadata": {},
   "source": [
    "## **5. PGD Attack ($\\ell_\\infty$ and $\\ell_2$)** <a name=\"pgd\"></a>\n",
    "\n",
    "Projected Gradient Descent (PGD) iterates FGSM steps and projects back into the $\\epsilon$-ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da418f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model, x, y, epsilon=0.3, alpha=0.01, steps=40):\n",
    "    x = x.clone().detach().to(device)\n",
    "    y = y.to(device)\n",
    "    x_adv = x + 0.001 * torch.randn_like(x)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad = True\n",
    "        logits = model(x_adv)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = x_adv.grad.sign()\n",
    "\n",
    "        x_adv = x_adv + alpha * grad\n",
    "        perturbation = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)\n",
    "        x_adv = torch.clamp(x + perturbation, min_val, max_val).detach()\n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762898a",
   "metadata": {},
   "source": [
    "### **Robustness Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b819d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_accuracy(model, loader, attack_fn, **kwargs):\n",
    "    correct, total = 0, 0\n",
    "    model.eval()\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        x_adv = attack_fn(model, x, y, **kwargs)\n",
    "        logits = model(x_adv)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "fgsm_acc = adversarial_accuracy(model, test_loader, fgsm_attack, epsilon=0.3)\n",
    "pgd_acc = adversarial_accuracy(model, test_loader, pgd_attack, epsilon=0.3, alpha=0.01, steps=20)\n",
    "\n",
    "print(f\"FGSM accuracy (eps=0.3): {fgsm_acc * 100:.2f}%\")\n",
    "print(f\"PGD accuracy (eps=0.3):  {pgd_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab384e",
   "metadata": {},
   "source": [
    "### **Visual Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6bd4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize adversarial examples\n",
    "x, y = next(iter(test_loader))\n",
    "\n",
    "x_adv_fgsm = fgsm_attack(model, x[:5], y[:5], epsilon=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i in range(5):\n",
    "    axes[0, i].imshow((x[i].squeeze() * 0.3081 + 0.1307).cpu(), cmap='gray')\n",
    "    axes[0, i].set_title(f\"Clean: {y[i].item()}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    axes[1, i].imshow((x_adv_fgsm[i].squeeze() * 0.3081 + 0.1307).cpu(), cmap='gray')\n",
    "    axes[1, i].set_title(\"FGSM\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89e5d8",
   "metadata": {},
   "source": [
    "### **$\\ell_2$-Norm PGD Attack**\n",
    "\n",
    "The standard PGD constrains perturbation to an $\\ell_\\infty$-ball constraint. Another common perturbation constraint is the $\\ell_2$-norm, where the Euclidean distance between the original output and adversarial example is bounded by $\\epsilon$.\n",
    "\n",
    "Let's implement the helpers and an $\\ell_2$ version of PGD manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8549bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_l2(x_batch):\n",
    "    \"\"\"Normalizes a batch of images by their L2 norm.\"\"\"\n",
    "    # L2 norm over dimensions 1, 2, 3 (C, H, W)\n",
    "    norm = torch.norm(x_batch.view(x_batch.size(0), -1), p=2, dim=1, keepdim=True)\n",
    "    norm = norm.unsqueeze(2).unsqueeze(3) # Reshape to match input for broadcasting\n",
    "    # Avoid division by zero\n",
    "    norm = torch.clamp(norm, min=1e-8)\n",
    "    return x_batch / norm\n",
    "\n",
    "def tensor_clamp_l2(x_batch, center, radius):\n",
    "    \"\"\"Batched clamp of x into l2 ball around center of given radius.\"\"\"\n",
    "    diff = x_batch - center\n",
    "    diff_norm = torch.norm(diff.view(diff.size(0), -1), p=2, dim=1, keepdim=True)\n",
    "    diff_norm = diff_norm.unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "    # Scale back if norm exceeds radius\n",
    "    factor = torch.min(torch.ones_like(diff_norm), radius / (diff_norm + 1e-8))\n",
    "    return center + diff * factor\n",
    "\n",
    "def pgd_l2_attack(model, x, y, epsilon=3.0, step_size=0.5, steps=20):\n",
    "    x = x.clone().detach().to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    # Initialize adversarial image with small noise\n",
    "    x_adv = x.clone().detach() + 0.001 * torch.randn_like(x)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad = True\n",
    "        logits = model(x_adv)\n",
    "        loss = nn.CrossEntropyLoss()(logits, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take a step in the gradient direction, normalized by L2\n",
    "        grad = normalize_l2(x_adv.grad)\n",
    "        x_adv = x_adv + step_size * grad\n",
    "        \n",
    "        # Project (by clamping) the adversarial image back onto the L2 hypersphere\n",
    "        x_adv = tensor_clamp_l2(x_adv, x, epsilon)\n",
    "        x_adv = torch.clamp(x_adv, min_val, max_val).detach()\n",
    "        \n",
    "    return x_adv\n",
    "\n",
    "pgd_l2_acc = adversarial_accuracy(model, test_loader, pgd_l2_attack, epsilon=3.0, step_size=0.5, steps=20)\n",
    "print(f\"PGD L2 accuracy (eps=3.0): {pgd_l2_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e92d5",
   "metadata": {},
   "source": [
    "### **Accuracy vs Number of PGD Steps**\n",
    "Often, limiting the attacker to just a few steps prevents them from finding the optimal adversarial perturbation. As we increase the number of iteration steps, the attack becomes stronger, finding examples that reliably fool the model. Let's see how accuracy degrades as steps increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_counts = [1, 5, 10, 20, 50]\n",
    "accuracies = []\n",
    "\n",
    "print(\"Running evaluation over multiple steps...\")\n",
    "for steps in step_counts:\n",
    "    acc = adversarial_accuracy(model, test_loader, pgd_attack, epsilon=0.3, alpha=0.01, steps=steps)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Steps={steps} -> Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(step_counts, [a * 100 for a in accuracies], marker='o')\n",
    "plt.title(\"Model Accuracy vs. PGD Steps\")\n",
    "plt.xlabel(\"Number of PGD Steps\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c92b0",
   "metadata": {},
   "source": [
    "## **6. Targeted Evasion Attacks** <a name=\"targeted\"></a>\n",
    "\n",
    "In the previous sections, we performed **untargeted** attacks, where the goal was simply to make the model misclassify the input. In a **targeted** attack, the adversary wants the model to output a *specific* incorrect class.\n",
    "\n",
    "The optimization objective changes from maximizing the loss of the true class to minimizing the loss of a target class $y_{target}$:\n",
    "\n",
    "$$\\min_{\\delta} \\mathcal{L}(f(x + \\delta), y_{target}) \\quad \\text{s.t.} \\quad \\|\\delta\\|_p \\le \\epsilon$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_fgsm_attack(model, images, target_labels, epsilon):\n",
    "    images = images.clone().detach().to(device)\n",
    "    target_labels = target_labels.to(device)\n",
    "    images.requires_grad = True\n",
    "    \n",
    "    outputs = model(images)\n",
    "    # We MINIMIZE the loss relative to the target label\n",
    "    loss = nn.CrossEntropyLoss()(outputs, target_labels)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient Descent (not Ascent) to move towards the target\n",
    "    data_grad = images.grad.data\n",
    "    perturbed_image = images - epsilon * data_grad.sign()\n",
    "    \n",
    "    # Maintain normalization bounds for MNIST\n",
    "    perturbed_image = torch.clamp(perturbed_image, min_val, max_val)\n",
    "    \n",
    "    return perturbed_image\n",
    "\n",
    "# Example: Pick a '3' and try to make it look like an '8'\n",
    "sample_idx = 7 # Adjust to find a '3' in your batch\n",
    "x_sample, y_sample = x[sample_idx:sample_idx+1], y[sample_idx:sample_idx+1]\n",
    "y_target = torch.tensor([8]).to(device)\n",
    "\n",
    "x_adv_targeted = targeted_fgsm_attack(model, x_sample, y_target, epsilon=0.3)\n",
    "\n",
    "# Check prediction\n",
    "output_adv = model(x_adv_targeted)\n",
    "pred_adv = torch.argsort(output_adv, descending=True)[0]\n",
    "\n",
    "print(f\"True Label: {y_sample.item()}\")\n",
    "print(f\"Target Label: {y_target.item()}\")\n",
    "print(f\"Top 3 Predictions: {pred_adv[:3].cpu().numpy()}\")\n",
    "\n",
    "plt.imshow((x_adv_targeted[0].squeeze() * 0.3081 + 0.1307).cpu().detach(), cmap='gray')\n",
    "plt.title(f\"Targeted FGSM (Target: {y_target.item()})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd9fc5",
   "metadata": {},
   "source": [
    "### **Targeted PGD Attack**\n",
    "Targeted FGSM often fails due to a lack of iterative refinement. Let's implement Targeted PGD, which applies gradient descent (subtracting the sign of the gradient relative to the target label) iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targeted_pgd_attack(model, x, target_labels, epsilon=0.3, alpha=0.01, steps=40):\n",
    "    x = x.clone().detach().to(device)\n",
    "    target_labels = target_labels.to(device)\n",
    "    \n",
    "    x_adv = x + 0.001 * torch.randn_like(x)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad = True\n",
    "        logits = model(x_adv)\n",
    "        \n",
    "        # We MINIMIZE the loss relative to the target label\n",
    "        loss = nn.CrossEntropyLoss()(logits, target_labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        grad = x_adv.grad.sign()\n",
    "        \n",
    "        # Gradient Descent (not Ascent) to move towards the target\n",
    "        x_adv = x_adv - alpha * grad\n",
    "        perturbation = torch.clamp(x_adv - x, min=-epsilon, max=epsilon)\n",
    "        x_adv = torch.clamp(x + perturbation, min_val, max_val).detach()\n",
    "        \n",
    "    return x_adv\n",
    "\n",
    "x_adv_targeted_pgd = targeted_pgd_attack(model, x_sample, y_target, epsilon=0.3, steps=100)\n",
    "\n",
    "# Check prediction\n",
    "output_adv_pgd = model(x_adv_targeted_pgd)\n",
    "pred_adv_pgd = torch.argsort(output_adv_pgd, descending=True)[0]\n",
    "\n",
    "print(\"--- Targeted PGD ---\")\n",
    "print(f\"True Label: {y_sample.item()}\")\n",
    "print(f\"Target Label: {y_target.item()}\")\n",
    "print(f\"Top 3 Predictions: {pred_adv_pgd[:3].cpu().numpy()}\")\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow((x_adv_targeted_pgd[0].squeeze() * 0.3081 + 0.1307).cpu().detach(), cmap='gray')\n",
    "plt.title(f\"Targeted PGD (Target: {y_target.item()})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860c891",
   "metadata": {},
   "source": [
    "## **7. Adversarial Training (Defense)** <a name=\"defense\"></a>\n",
    "\n",
    "Adversarial training is one of the most effective and classical defenses against adversarial attacks. The concept is straightforward: generate adversarial examples during the training process and use them to augment the training dataset. Over time, the model learns to map these perturbed inputs to their correct labels, smoothing its decision boundaries.\n",
    "\n",
    "Let's train a robust version of our model by including adversarial examples in the training loop. We will use the `secmlt` library's PGD attack during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d46740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a robust model\n",
    "robust_net = MNISTNet().to(device)\n",
    "optimizer_robust = torch.optim.Adam(robust_net.parameters(), lr=0.001)\n",
    "\n",
    "# PGD attack for generating adversarial examples during training\n",
    "pgd_train = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=7,\n",
    "    step_size=0.1,\n",
    "    random_start=True,\n",
    "    lb=min_val,\n",
    "    ub=max_val,\n",
    "    backend=Backends.NATIVE\n",
    ")\n",
    "\n",
    "print(\"Starting Adversarial Training (1 epoch)...\")\n",
    "robust_net.train()\n",
    "\n",
    "# Manual Adversarial Training Loop\n",
    "for x_batch, y_batch in tqdm(train_loader):\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    \n",
    "    # 1. Generate adversarial examples for the current batch\n",
    "    # We create a temporary DataLoader for the current batch to satisfy secmlt's requirements\n",
    "    tmp_model = BasePytorchClassifier(model=robust_net)\n",
    "    batch_loader = DataLoader(TensorDataset(x_batch, y_batch), batch_size=x_batch.size(0))\n",
    "    adv_loader = pgd_train(tmp_model, batch_loader)\n",
    "    x_adv, _ = next(iter(adv_loader))\n",
    "    \n",
    "    # 2. Standard training step on the adversarial examples\n",
    "    optimizer_robust.zero_grad()\n",
    "    outputs = robust_net(x_adv.to(device))\n",
    "    loss = nn.CrossEntropyLoss()(outputs, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer_robust.step()\n",
    "\n",
    "print(\"Robust training complete.\")\n",
    "\n",
    "# Evaluate this model's clean accuracy vs adversarial accuracy\n",
    "robust_net.eval()\n",
    "secml_robust_model = BasePytorchClassifier(model=robust_net)\n",
    "\n",
    "clean_acc_robust = Accuracy()(secml_robust_model, test_loader)\n",
    "\n",
    "# Use a strong 20-step PGD to test robustness\n",
    "pgd_eval = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=20,\n",
    "    step_size=0.05,\n",
    "    lb=min_val,\n",
    "    ub=max_val,\n",
    "    backend=Backends.NATIVE\n",
    ")\n",
    "adv_loader_robust = pgd_eval(secml_robust_model, test_loader)\n",
    "robust_acc_robust = Accuracy()(secml_robust_model, adv_loader_robust)\n",
    "\n",
    "print(f\"\\nRobust Model - Clean Accuracy: {clean_acc_robust * 100:.2f}%\")\n",
    "print(f\"Robust Model - PGD Accuracy (eps=0.3): {robust_acc_robust * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4013592",
   "metadata": {},
   "source": [
    "## **8. Library Implementation (secml-torch)** <a name=\"library\"></a>\n",
    "\n",
    "Now that we have implemented manual FGSM/PGD, we can replicate the same attacks using the `secml-torch` library for standardized evaluation and comparison.\n",
    "\n",
    "Using a library like `secml-torch` simplifies research by providing standardized implementations of attacks and robust evaluation metrics.\n",
    "\n",
    "### **7.1 Untargeted PGD with secml-torch**\n",
    "We'll use the `PGD` class from `secml-torch`. Note that we must specify the `lb` (lower bound) and `ub` (upper bound) to match our normalized data range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2236c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from secmlt.models.pytorch.base_pytorch_nn import BasePytorchClassifier\n",
    "from secmlt.adv.evasion.pgd import PGD\n",
    "from secmlt.adv.backends import Backends\n",
    "from secmlt.adv.evasion.perturbation_models import LpPerturbationModels\n",
    "from secmlt.metrics.classification import Accuracy, AttackSuccessRate\n",
    "\n",
    "# Wrap our model\n",
    "secml_model = BasePytorchClassifier(model)\n",
    "\n",
    "# Define bounds based on MNIST normalization\n",
    "# Reuse min_val and max_val computed earlier from mean/std\n",
    "\n",
    "# Instantiate PGD\n",
    "# To simulate FGSM, we can use num_steps=2\n",
    "pgd_attack = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=10,\n",
    "    step_size=0.05,\n",
    "    random_start=False,\n",
    "    backend=Backends.NATIVE,\n",
    "    lb=min_val,\n",
    "    ub=max_val\n",
    ")\n",
    "\n",
    "# Run attack on a small batch from our test_loader\n",
    "adv_loader = pgd_attack(secml_model, test_loader)\n",
    "\n",
    "# Evaluate Robust Accuracy\n",
    "acc_metric = Accuracy()\n",
    "robust_acc = acc_metric(secml_model, adv_loader)\n",
    "print(f\"Robust Accuracy (PGD): {robust_acc.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ef5eb",
   "metadata": {},
   "source": [
    "\n",
    "### **7.2 Targeted Attack and Attack Success Rate (ASR)**\n",
    "The **Attack Success Rate (ASR)** measures the percentage of samples that the attacker successfully pushed to the target class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef40617",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_target_val = 8\n",
    "pgd_targeted = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=15,\n",
    "    step_size=0.05,\n",
    "    y_target=y_target_val,\n",
    "    backend=Backends.NATIVE,\n",
    "    lb=min_val,\n",
    "    ub=max_val\n",
    ")\n",
    "\n",
    "adv_loader_targeted = pgd_targeted(secml_model, test_loader)\n",
    "\n",
    "# Calculate ASR\n",
    "asr_metric = AttackSuccessRate(y_target=y_target_val)\n",
    "asr = asr_metric(secml_model, adv_loader_targeted)\n",
    "print(f\"Attack Success Rate (Target: {y_target_val}): {asr.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bdb52",
   "metadata": {},
   "source": [
    "## **9. Exercises** <a name=\"exercises\"></a>\n",
    "\n",
    "1. Run FGSM with $\\epsilon \\in \\{0.05, 0.1, 0.2, 0.3\\}$ and plot accuracy vs. $\\epsilon$.  \n",
    "2. Compare the perturbations visually between $\\ell_\\infty$ and $\\ell_2$ attacks. You can display the difference array, e.g., `(x_adv - x).abs()`.  \n",
    "3. Train an adversarially robust model using **PGD** instead of FGSM for 1 epoch and evaluate its robustness. How does training time change? \n",
    "4. Discuss why PGD is generally stronger than FGSM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
