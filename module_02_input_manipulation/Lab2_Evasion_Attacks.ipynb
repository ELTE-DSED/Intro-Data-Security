{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c5b20eb",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ELTE-DSED/Intro-Data-Security/blob/main/module_02_input_manipulation/Lab2_Evasion_Attacks.ipynb)\n",
    "\n",
    "# **Lab 2: Evasion Attacks (Adversarial Examples)**\n",
    "\n",
    "**Course:** Introduction to Data Security Pr.  \n",
    "**Module 2:** Input Data Manipulation  \n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "---\n",
    "<div align=\"center\">\n",
    "  <img src=\"images/openai_panda.png\" alt=\"Adversarial Example: Panda to Gibbon\">\n",
    "</div>\n",
    "\n",
    "In this lab, we explore how adversaries can manipulate input data to deceive machine learning models. Adversarial examples are carefully crafted inputs that appear normal to humans but cause models to make incorrect predictions with high confidence. These attacks highlight vulnerabilities in even state-of-the-art neural networks and motivate the need for robust defenses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccadae0",
   "metadata": {},
   "source": [
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "- Generate adversarial examples using FGSM and PGD\n",
    "- Measure model robustness under $\\ell_\\infty$ perturbations\n",
    "- Visualize perturbations and their impact on predictions\n",
    "- Compare clean vs. adversarial accuracy\n",
    "- Discuss the robustnessâ€“accuracy tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3d3e3",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "1. [Setup & Imports](#setup)  \n",
    "2. [Load Model & Dataset](#data)  \n",
    "3. [Fast Gradient Sign Method (FGSM)](#fgsm)  \n",
    "4. [PGD Attack ($\\ell_\\infty$ and $\\ell_2$)](#pgd)  \n",
    "5. [Adversarial Training (Defense)](#defense)  \n",
    "6. [Library Implementation (secml-torch)](#library)  \n",
    "7. [Exercises](#exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111b8bb",
   "metadata": {},
   "source": [
    "## **1. Setup & Imports** <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install dependencies\n",
    "%pip install secml-torch -q\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "if not os.path.exists('utils.py'):\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/ELTE-DSED/Intro-Data-Security/main/module_02_input_manipulation/utils.py', 'utils.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662a648f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:19.194711Z",
     "iopub.status.busy": "2026-02-23T00:57:19.194555Z",
     "iopub.status.idle": "2026-02-23T00:57:22.418158Z",
     "shell.execute_reply": "2026-02-23T00:57:22.416963Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing all the necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils import test_untargeted_attack, test_targeted_attack\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85042f4a",
   "metadata": {},
   "source": [
    "## **2. Load Model & Dataset** <a name=\"data\"></a>\n",
    "\n",
    "We will train MNIST model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595c9f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:22.420421Z",
     "iopub.status.busy": "2026-02-23T00:57:22.420166Z",
     "iopub.status.idle": "2026-02-23T00:57:22.535217Z",
     "shell.execute_reply": "2026-02-23T00:57:22.533896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load MNIST test set\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Optional train set for fallback training\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define the MLP model from Lab 1\n",
    "class MNISTNet(torch.nn.Module):\n",
    "    \"\"\"Simple fully connected network for MNIST classification.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(784, 200)\n",
    "        self.fc2 = torch.nn.Linear(200, 200)\n",
    "        self.fc3 = torch.nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "net = MNISTNet().to(device)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# Normalized clamp bounds for MNIST\n",
    "mnist_mean = 0.1307\n",
    "mnist_std = 0.3081\n",
    "min_val = (0.0 - mnist_mean) / mnist_std\n",
    "max_val = (1.0 - mnist_mean) / mnist_std\n",
    "print(f\"Normalized clamp range: [{min_val:.2f}, {max_val:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d40cf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:22.537734Z",
     "iopub.status.busy": "2026-02-23T00:57:22.537490Z",
     "iopub.status.idle": "2026-02-23T00:57:30.871571Z",
     "shell.execute_reply": "2026-02-23T00:57:30.869935Z"
    }
   },
   "outputs": [],
   "source": [
    "from secmlt.models.pytorch.base_pytorch_nn import BasePytorchClassifier\n",
    "from secmlt.models.pytorch.base_pytorch_trainer import BasePyTorchTrainer\n",
    "from secmlt.metrics.classification import Accuracy\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the trainer for 1 epoch\n",
    "trainer = BasePyTorchTrainer(optimizer=optimizer, epochs=1)\n",
    "\n",
    "# Wrap the net and train\n",
    "model = BasePytorchClassifier(model=net, trainer=trainer)\n",
    "model.train(dataloader=train_loader)\n",
    "\n",
    "# Evaluate accuracy\n",
    "clean_accuracy = Accuracy()(model, test_loader)\n",
    "print(f\"Clean accuracy: {clean_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bfff0",
   "metadata": {},
   "source": [
    "## **3. FGSM Attack** <a name=\"fgsm\"></a>\n",
    "### **3.1 Untargeted FGSM**\n",
    "\n",
    "The Fast Gradient Sign Method (FGSM) proposed by [Goodfellow et al.](https://arxiv.org/pdf/1412.6572.pdf)  crafts adversarial examples by taking a single step in the direction of the input gradient.\n",
    "\n",
    "The attack constructs adversarial examples as follows:\n",
    "\n",
    "$$x_\\text{adv} = x + \\epsilon\\cdot\\text{sign}(\\nabla_xJ(\\theta, x, y))$$\n",
    "\n",
    "where \n",
    "\n",
    "*   $x_\\text{adv}$ : Adversarial image.\n",
    "*   $x$ : Original input image.\n",
    "*   $y$ : Original input label.\n",
    "*   $\\epsilon$ : Multiplier to ensure the perturbations are small.\n",
    "*   $\\theta$ : Model parameters.\n",
    "*   $J$ : Loss.\n",
    "\n",
    "The current attack formulation is considered *untargeted* because it only seeks to maximize loss rather than to trick the model into predicting a specific label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6f6464",
   "metadata": {},
   "source": [
    "Below is a complete implementation of the untargeted FGSM attack to understand how gradients from the model are used to perturb the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426ceaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:30.874687Z",
     "iopub.status.busy": "2026-02-23T00:57:30.874342Z",
     "iopub.status.idle": "2026-02-23T00:57:30.894700Z",
     "shell.execute_reply": "2026-02-23T00:57:30.893481Z"
    }
   },
   "outputs": [],
   "source": [
    "def untargeted_fgsm(x_batch, true_labels, network, normalize, eps=8/255., **kwargs):\n",
    "  \"\"\"\n",
    "  Generates a batch of untargeted FGSM adversarial examples.\n",
    "  (Worked Example)\n",
    "  \"\"\"\n",
    "  \n",
    "  # 1. Clone input and enable gradient tracking\n",
    "  x_batch = x_batch.clone().detach().requires_grad_(True)\n",
    "  \n",
    "  # 2. Normalize images for the network\n",
    "  x_normalized = normalize(x_batch)\n",
    "  \n",
    "  # 3. Forward pass\n",
    "  logits = network(x_normalized)\n",
    "  \n",
    "  # 4. Compute Loss\n",
    "  loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "  loss = loss_fn(logits, true_labels)\n",
    "  \n",
    "  # 5. Backward pass to get gradients\n",
    "  network.zero_grad()\n",
    "  loss.backward()\n",
    "  \n",
    "  # 6. Extract the sign of the gradient\n",
    "  grad_sign = x_batch.grad.data.sign()\n",
    "  \n",
    "  # 7. Create adversarial images (FGSM formula: x + eps * sign(grad))\n",
    "  x_adv = x_batch + eps * grad_sign\n",
    "  \n",
    "  # 8. Clamp back to valid pixel range [0, 1]\n",
    "  x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "  \n",
    "  return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f259a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:30.897173Z",
     "iopub.status.busy": "2026-02-23T00:57:30.896891Z",
     "iopub.status.idle": "2026-02-23T00:57:31.000676Z",
     "shell.execute_reply": "2026-02-23T00:57:30.998288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the method\n",
    "test_untargeted_attack(untargeted_fgsm, net, device, eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3f51b",
   "metadata": {},
   "source": [
    "### **3.2 Targeted FGSM**\n",
    "\n",
    "In addition to the untargeted FGSM which simply seeks to maximize loss, we can also create targeted adversarial attacks. We do this using the following equation:\n",
    "\n",
    "$$x_{adv} = x - \\epsilon\\cdot\\text{sign}(\\nabla_xJ(\\theta, x, y_{target}))$$\n",
    "\n",
    "where \n",
    "\n",
    "*   $x_{adv}$ : Adversarial image.\n",
    "*   $x$ : Original input image.\n",
    "*   $y_{target}$ : The target label.\n",
    "*   $\\epsilon$ : Multiplier to ensure the perturbations are small.\n",
    "*   $\\theta$ : Model parameters.\n",
    "*   $J$ : Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a7caa",
   "metadata": {},
   "source": [
    "Now it's your turn! Implement the `targeted_fgsm` function below. \n",
    "\n",
    "**Hint:** Instead of moving in a direction that *increases* the loss of the true label, you should move in a direction that *decreases* the loss of the **target label**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4db5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.002746Z",
     "iopub.status.busy": "2026-02-23T00:57:31.002538Z",
     "iopub.status.idle": "2026-02-23T00:57:31.019977Z",
     "shell.execute_reply": "2026-02-23T00:57:31.018769Z"
    }
   },
   "outputs": [],
   "source": [
    "def targeted_fgsm(x_batch, target_labels, network, normalize, eps=8/255., **kwargs):\n",
    "  \"\"\"\n",
    "  Generates a batch of targeted FGSM adversarial examples.\n",
    "  \"\"\"\n",
    "  # 1. Clone input and enable gradient tracking\n",
    "  x_batch = x_batch.clone().detach().requires_grad_(True)\n",
    "\n",
    "  # 2. Normalize images for the network\n",
    "  x_normalized = normalize(x_batch)\n",
    "  \n",
    "  # 3. Forward pass\n",
    "  logits = network(x_normalized)\n",
    "  \n",
    "  # 4. Compute Loss relative to target labels\n",
    "  loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "  loss = loss_fn(logits, target_labels)\n",
    "  \n",
    "  # 5. Backward pass\n",
    "  network.zero_grad()\n",
    "  loss.backward()\n",
    "  \n",
    "  # 6. Extract the sign of the gradient\n",
    "  grad_sign = x_batch.grad.data.sign()\n",
    "  \n",
    "  # 7. Targeted update: SUBTRACT gradient sign to MINIMIZE loss of target\n",
    "  x_adv = x_batch - eps * grad_sign\n",
    "  \n",
    "  # 8. Clamp back to valid pixel range [0, 1]\n",
    "  x_adv = torch.clamp(x_adv, 0.0, 1.0)\n",
    "  \n",
    "  return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4db5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.021667Z",
     "iopub.status.busy": "2026-02-23T00:57:31.021441Z",
     "iopub.status.idle": "2026-02-23T00:57:31.114349Z",
     "shell.execute_reply": "2026-02-23T00:57:31.112480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your targeted implementation\n",
    "test_targeted_attack(targeted_fgsm, net, device, target_label=2, eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7e40c",
   "metadata": {},
   "source": [
    "## **4. PGD Attack ($\\ell_\\infty$ and $\\ell_2$)** <a name=\"pgd\"></a>\n",
    "\n",
    "### **4.1 Untargeted PGD**\n",
    "\n",
    "Projected Gradient Descent (PGD) by [Madry et al.](https://arxiv.org/pdf/1706.06083.pdf) iterates FGSM steps and projects back into the $\\epsilon$-ball. Specifically, the adversarial example $x^{t+1}$ at step $t+1$ is computed as:\\n\\n$$x^{t+1} = \\Pi_{x+\\mathcal{S}} (x^t + \\alpha \\cdot \\text{sign}(\\nabla_x J(\\theta, x^t, y)))$$\\n\\nwhere $\\Pi_{x+\\mathcal{S}}$ is the projection operation that ensures the perturbed image remains within the valid pixel range and the allowed $\\epsilon$-ball $\\mathcal{S}$ around the original image $x$, and $\\alpha$ is the step size. \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da418f67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.117213Z",
     "iopub.status.busy": "2026-02-23T00:57:31.116811Z",
     "iopub.status.idle": "2026-02-23T00:57:31.136396Z",
     "shell.execute_reply": "2026-02-23T00:57:31.135192Z"
    }
   },
   "outputs": [],
   "source": [
    "def untargeted_pgd(x_batch, true_labels, network, normalize, eps=0.3, alpha=0.01, steps=40, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a batch of untargeted PGD adversarial examples.\n",
    "    \"\"\"\n",
    "    x_batch = x_batch.clone().detach().to(device)\n",
    "    true_labels = true_labels.to(device)\n",
    "    \n",
    "    # Initialize adversarial image with small noise\n",
    "    x_adv = x_batch + 0.001 * torch.randn_like(x_batch)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        \n",
    "        # Normalize images for the network\n",
    "        x_normalized = normalize(x_adv)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = network(x_normalized)\n",
    "        \n",
    "        # Compute Loss\n",
    "        loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        loss = loss_fn(logits, true_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        network.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract the sign of the gradient\n",
    "        grad_sign = x_adv.grad.data.sign()\n",
    "\n",
    "        # Update adversarial image\n",
    "        x_adv = x_adv + alpha * grad_sign\n",
    "        \n",
    "        # Project perturbation back into epsilon-ball\n",
    "        perturbation = torch.clamp(x_adv - x_batch, min=-eps, max=eps)\n",
    "        \n",
    "        # Apply perturbation and clamp to valid pixel range [0, 1]\n",
    "        x_adv = torch.clamp(x_batch + perturbation, 0.0, 1.0).detach()\n",
    "        \n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h3c18f15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.138244Z",
     "iopub.status.busy": "2026-02-23T00:57:31.138078Z",
     "iopub.status.idle": "2026-02-23T00:57:31.240299Z",
     "shell.execute_reply": "2026-02-23T00:57:31.239109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the method\n",
    "test_untargeted_attack(untargeted_pgd, net, device, eps=0.3, alpha=0.01, steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2898a",
   "metadata": {},
   "source": [
    "### **4.2 Targeted PGD**\n",
    "\n",
    "Now implement the `targeted_pgd` function below.\n",
    "\n",
    "**Hint:** Similar to targeted FGSM, you should move in a direction that *decreases* the loss of the **target label**. Remember to apply the step `alpha` instead of `eps` in each iteration, and project the total perturbation to the `eps`-ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a19d16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.242285Z",
     "iopub.status.busy": "2026-02-23T00:57:31.242118Z",
     "iopub.status.idle": "2026-02-23T00:57:31.259866Z",
     "shell.execute_reply": "2026-02-23T00:57:31.258725Z"
    }
   },
   "outputs": [],
   "source": [
    "def targeted_pgd(x_batch, target_labels, network, normalize, eps=0.3, alpha=0.01, steps=40, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates a batch of targeted PGD adversarial examples.\n",
    "    \"\"\"\n",
    "    x_batch = x_batch.clone().detach().to(device)\n",
    "    target_labels = target_labels.to(device)\n",
    "    \n",
    "    # Initialize adversarial image with small noise\n",
    "    x_adv = x_batch + 0.001 * torch.randn_like(x_batch)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad_(True)\n",
    "        \n",
    "        # Normalize images for the network\n",
    "        x_normalized = normalize(x_adv)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = network(x_normalized)\n",
    "        \n",
    "        # Compute Loss relative to target labels\n",
    "        loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        loss = loss_fn(logits, target_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        network.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Extract the sign of the gradient\n",
    "        grad_sign = x_adv.grad.data.sign()\n",
    "\n",
    "        # Targeted update: SUBTRACT gradient sign to MINIMIZE loss of target\n",
    "        x_adv = x_adv - alpha * grad_sign\n",
    "        \n",
    "        # Project perturbation back into epsilon-ball\n",
    "        perturbation = torch.clamp(x_adv - x_batch, min=-eps, max=eps)\n",
    "        \n",
    "        # Apply perturbation and clamp to valid pixel range [0, 1]\n",
    "        x_adv = torch.clamp(x_batch + perturbation, 0.0, 1.0).detach()\n",
    "        \n",
    "    return x_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g9b29e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.261923Z",
     "iopub.status.busy": "2026-02-23T00:57:31.261743Z",
     "iopub.status.idle": "2026-02-23T00:57:31.361308Z",
     "shell.execute_reply": "2026-02-23T00:57:31.360159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test your targeted implementation\n",
    "test_targeted_attack(targeted_pgd, net, device, target_label=2, eps=0.3, alpha=0.01, steps=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89e5d8",
   "metadata": {},
   "source": [
    "### **4.3 $\\ell_2$-Norm PGD Attack**\n",
    "\n",
    "The standard PGD constrains perturbation to an $\\ell_\\infty$-ball constraint. Another common perturbation constraint is the $\\ell_2$-norm, where the Euclidean distance between the original output and adversarial example is bounded by $\\epsilon$.\n",
    "\n",
    "Let's implement the helpers and an $\\ell_2$ version of PGD manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adv_acc_cell",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.363570Z",
     "iopub.status.busy": "2026-02-23T00:57:31.363401Z",
     "iopub.status.idle": "2026-02-23T00:57:31.379565Z",
     "shell.execute_reply": "2026-02-23T00:57:31.378422Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import MNIST_NORMALIZE, mnist_denormalize\n",
    "\n",
    "def adversarial_accuracy(network, dataloader, adv_attack, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of the model on adversarial examples.\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # The test_loader yields normalized images, we must unnormalize to [0,1] for our attacks\n",
    "        x_unnorm = mnist_denormalize(x_batch).to(device)\n",
    "        \n",
    "        # Generate adversarial images (the attack function will normalize internally)\n",
    "        x_adv = adv_attack(x_unnorm, y_batch, network, MNIST_NORMALIZE, **kwargs)\n",
    "        \n",
    "        # Normalize the output for the network classification\n",
    "        x_adv_norm = MNIST_NORMALIZE(x_adv)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = network(x_adv_norm)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8549bbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:31.381923Z",
     "iopub.status.busy": "2026-02-23T00:57:31.381617Z",
     "iopub.status.idle": "2026-02-23T00:57:37.672406Z",
     "shell.execute_reply": "2026-02-23T00:57:37.670332Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_l2(x_batch):\n",
    "    \"\"\"Normalizes a batch of images by their L2 norm.\"\"\"\n",
    "    # L2 norm over dimensions 1, 2, 3 (C, H, W)\n",
    "    norm = torch.norm(x_batch.view(x_batch.size(0), -1), p=2, dim=1, keepdim=True)\n",
    "    norm = norm.unsqueeze(2).unsqueeze(3) # Reshape to match input for broadcasting\n",
    "    # Avoid division by zero\n",
    "    norm = torch.clamp(norm, min=1e-8)\n",
    "    return x_batch / norm\n",
    "\n",
    "def tensor_clamp_l2(x_batch, center, radius):\n",
    "    \"\"\"Batched clamp of x into l2 ball around center of given radius.\"\"\"\n",
    "    diff = x_batch - center\n",
    "    diff_norm = torch.norm(diff.view(diff.size(0), -1), p=2, dim=1, keepdim=True)\n",
    "    diff_norm = diff_norm.unsqueeze(2).unsqueeze(3)\n",
    "    \n",
    "    # Scale back if norm exceeds radius\n",
    "    factor = torch.min(torch.ones_like(diff_norm), radius / (diff_norm + 1e-8))\n",
    "    return center + diff * factor\n",
    "\n",
    "def pgd_l2_attack(x_batch, true_labels, network, normalize, eps=3.0, alpha=0.5, steps=20, **kwargs):\n",
    "    x_batch = x_batch.clone().detach().to(device)\n",
    "    true_labels = true_labels.to(device)\n",
    "    \n",
    "    # Initialize adversarial image with small noise\n",
    "    x_adv = x_batch.clone().detach() + 0.001 * torch.randn_like(x_batch)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        x_adv.requires_grad = True\n",
    "        x_normalized = normalize(x_adv)\n",
    "        logits = network(x_normalized)\n",
    "        loss = nn.CrossEntropyLoss()(logits, true_labels)\n",
    "        network.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Take a step in the gradient direction, normalized by L2\n",
    "        grad = normalize_l2(x_adv.grad)\n",
    "        x_adv = x_adv + alpha * grad\n",
    "        \n",
    "        # Project (by clamping) the adversarial image back onto the L2 hypersphere\n",
    "        x_adv = tensor_clamp_l2(x_adv, x_batch, eps)\n",
    "        x_adv = torch.clamp(x_adv, min_val, max_val).detach()\n",
    "        \n",
    "    return x_adv\n",
    "\n",
    "pgd_l2_acc = adversarial_accuracy(net, test_loader, pgd_l2_attack, eps=2.0, alpha=0.5, steps=20)\n",
    "print(f\"PGD L2 accuracy (eps=2.0): {pgd_l2_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93e92d5",
   "metadata": {},
   "source": [
    "### **4.4 Accuracy vs Number of PGD Steps**\n",
    "Often, limiting the attacker to just a few steps prevents them from finding the optimal adversarial perturbation. As we increase the number of iteration steps, the attack becomes stronger, finding examples that reliably fool the model. Let's see how accuracy degrades as steps increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a4cbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:57:37.674929Z",
     "iopub.status.busy": "2026-02-23T00:57:37.674659Z",
     "iopub.status.idle": "2026-02-23T00:58:01.997316Z",
     "shell.execute_reply": "2026-02-23T00:58:01.996027Z"
    }
   },
   "outputs": [],
   "source": [
    "step_counts = [1, 5, 10, 20, 50]\n",
    "accuracies = []\n",
    "\n",
    "print(\"Running evaluation over multiple steps...\")\n",
    "for steps in step_counts:\n",
    "    acc = adversarial_accuracy(net, test_loader, untargeted_pgd, eps=0.3, alpha=0.01, steps=steps)\n",
    "    accuracies.append(acc)\n",
    "    print(f\"Steps={steps} -> Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(step_counts, [a * 100 for a in accuracies], marker='o')\n",
    "plt.title(\"Model Accuracy vs. PGD Steps\")\n",
    "plt.xlabel(\"Number of PGD Steps\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8c92b0",
   "metadata": {},
   "source": [
    "## **5. Adversarial Training (Defense) <a name=\"defense\"></a>**\n",
    "\n",
    "Adversarial training is one of the most effective and classical defenses against adversarial attacks. The concept is straightforward: generate adversarial examples during the training process and use them to augment the training dataset. Over time, the model learns to map these perturbed inputs to their correct labels, smoothing its decision boundaries.\n",
    "\n",
    "Let's train a robust version of our model by including adversarial examples in the training loop. We will use the `secmlt` library's PGD attack during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d46740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:58:01.999367Z",
     "iopub.status.busy": "2026-02-23T00:58:01.999191Z",
     "iopub.status.idle": "2026-02-23T00:58:30.907072Z",
     "shell.execute_reply": "2026-02-23T00:58:30.905685Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from secmlt.adv.evasion.pgd import PGD\n",
    "from secmlt.adv.backends import Backends\n",
    "from secmlt.adv.evasion.perturbation_models import LpPerturbationModels\n",
    "\n",
    "# Define a robust net\n",
    "robust_net = MNISTNet().to(device)\n",
    "optimizer_robust = torch.optim.Adam(robust_net.parameters(), lr=0.001)\n",
    "\n",
    "# PGD attack for generating adversarial examples during training\n",
    "pgd_train = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=7,\n",
    "    step_size=0.1,\n",
    "    random_start=True,\n",
    "    lb=min_val,\n",
    "    ub=max_val,\n",
    "    backend=Backends.NATIVE\n",
    ")\n",
    "\n",
    "print(\"Starting Adversarial Training (1 epoch)...\")\n",
    "robust_net.train()\n",
    "\n",
    "# Manual Adversarial Training Loop\n",
    "for x_batch, y_batch in tqdm(train_loader):\n",
    "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "    \n",
    "    # 1. Generate adversarial examples for the current batch\n",
    "    # We create a temporary model for the current batch to satisfy secmlt's requirements\n",
    "    tmp_model = BasePytorchClassifier(model=robust_net)\n",
    "    batch_loader = DataLoader(TensorDataset(x_batch, y_batch), batch_size=x_batch.size(0))\n",
    "    adv_loader = pgd_train(tmp_model, batch_loader)\n",
    "    x_adv, _ = next(iter(adv_loader))\n",
    "    \n",
    "    # 2. Standard training step on the adversarial examples\n",
    "    optimizer_robust.zero_grad()\n",
    "    outputs = robust_net(x_adv.to(device))\n",
    "    loss = nn.CrossEntropyLoss()(outputs, y_batch)\n",
    "    loss.backward()\n",
    "    optimizer_robust.step()\n",
    "\n",
    "print(\"Robust training complete.\")\n",
    "\n",
    "# Evaluate this model's clean accuracy vs adversarial accuracy\n",
    "robust_net.eval()\n",
    "robust_model = BasePytorchClassifier(model=robust_net)\n",
    "\n",
    "clean_acc_robust = Accuracy()(robust_model, test_loader)\n",
    "\n",
    "# Use a strong 20-step PGD to test robustness\n",
    "pgd_eval = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=20,\n",
    "    step_size=0.05,\n",
    "    lb=min_val,\n",
    "    ub=max_val,\n",
    "    backend=Backends.NATIVE\n",
    ")\n",
    "adv_loader_robust = pgd_eval(robust_model, test_loader)\n",
    "robust_acc_robust = Accuracy()(robust_model, adv_loader_robust)\n",
    "\n",
    "print(f\"\\nRobust Model - Clean Accuracy: {clean_acc_robust * 100:.2f}%\")\n",
    "print(f\"Robust Model - PGD Accuracy (eps=0.3): {robust_acc_robust * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4013592",
   "metadata": {},
   "source": [
    "## **6. Library Implementation (secml-torch)** <a name=\"library\"></a>\n",
    "\n",
    "Now that we have implemented manual FGSM/PGD, we can replicate the same attacks using the `secml-torch` library for standardized evaluation and comparison.\n",
    "\n",
    "Using a library like `secml-torch` simplifies research by providing standardized implementations of attacks and robust evaluation metrics.\n",
    "\n",
    "### **6.1 Untargeted PGD with secml-torch**\n",
    "We'll use the `PGD` class from `secml-torch`. Note that we must specify the `lb` (lower bound) and `ub` (upper bound) to match our normalized data range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2236c1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:58:30.909799Z",
     "iopub.status.busy": "2026-02-23T00:58:30.909525Z",
     "iopub.status.idle": "2026-02-23T00:58:35.113980Z",
     "shell.execute_reply": "2026-02-23T00:58:35.111344Z"
    }
   },
   "outputs": [],
   "source": [
    "from secmlt.models.pytorch.base_pytorch_nn import BasePytorchClassifier\n",
    "from secmlt.adv.evasion.pgd import PGD\n",
    "from secmlt.adv.backends import Backends\n",
    "from secmlt.adv.evasion.perturbation_models import LpPerturbationModels\n",
    "from secmlt.metrics.classification import Accuracy, AttackSuccessRate\n",
    "\n",
    "# Wrap our net\n",
    "model = BasePytorchClassifier(net)\n",
    "\n",
    "# Define bounds based on MNIST normalization\n",
    "# Reuse min_val and max_val computed earlier from mean/std\n",
    "\n",
    "# Instantiate PGD\n",
    "# To simulate FGSM, we can use num_steps=2\n",
    "pgd_attack_secml = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=10,\n",
    "    step_size=0.05,\n",
    "    random_start=False,\n",
    "    backend=Backends.NATIVE,\n",
    "    lb=min_val,\n",
    "    ub=max_val\n",
    ")\n",
    "\n",
    "# Run attack on a small batch from our test_loader\n",
    "adv_loader = pgd_attack_secml(model, test_loader)\n",
    "\n",
    "# Evaluate Robust Accuracy\n",
    "acc_metric = Accuracy()\n",
    "robust_acc = acc_metric(model, adv_loader)\n",
    "print(f\"Robust Accuracy (PGD): {robust_acc.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ef5eb",
   "metadata": {},
   "source": [
    "### **6.2 Targeted Attack and Attack Success Rate (ASR)**\n",
    "The **Attack Success Rate (ASR)** measures the percentage of samples that the attacker successfully pushed to the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef40617",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T00:58:35.117769Z",
     "iopub.status.busy": "2026-02-23T00:58:35.117462Z",
     "iopub.status.idle": "2026-02-23T00:58:40.959295Z",
     "shell.execute_reply": "2026-02-23T00:58:40.957647Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "y_target_val = 8\n",
    "pgd_targeted = PGD(\n",
    "    perturbation_model=LpPerturbationModels.LINF,\n",
    "    epsilon=0.3,\n",
    "    num_steps=15,\n",
    "    step_size=0.05,\n",
    "    y_target=y_target_val,\n",
    "    backend=Backends.NATIVE,\n",
    "    lb=min_val,\n",
    "    ub=max_val\n",
    ")\n",
    "\n",
    "adv_loader_targeted = pgd_targeted(model, test_loader)\n",
    "\n",
    "# Calculate ASR\n",
    "asr_metric = AttackSuccessRate(y_target=y_target_val)\n",
    "asr = asr_metric(model, adv_loader_targeted)\n",
    "print(f\"Attack Success Rate (Target: {y_target_val}): {asr.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0bdb52",
   "metadata": {},
   "source": [
    "## **7. Exercises** <a name=\"exercises\"></a>\n",
    "\n",
    "1. Run FGSM with $\\epsilon \\in \\{0.05, 0.1, 0.2, 0.3\\}$ and plot accuracy vs. $\\epsilon$.\n",
    "2. Compare the perturbations visually between $\\ell_\\infty$ and $\\ell_2$ attacks. You can display the difference array, e.g., `(x_adv - x).abs()`.\n",
    "3. Train an adversarially robust model using **FGSM** instead of PGD for 1 epoch and evaluate its robustness against both FGSM and PGD attacks. How does training time change?\\n4. Discuss why PGD is generally stronger than FGSM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
