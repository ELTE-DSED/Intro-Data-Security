{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c36fa4",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ahmed-fouad-lagha/Intro-Data-Security/blob/main/module_01_foundations/Lab_1b_Threat_Modeling_and_Attack_Taxonomy.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be77aa",
   "metadata": {},
   "source": [
    "# **Lab 1b: Threat Modeling & Attack Taxonomy in AI Security**\n",
    "\n",
    "**Course:** Introduction to Data Security Pr. (Master's Level)  \n",
    "**Module 1:** Foundations  \n",
    "**Estimated Time:** 90-120 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Understand** the fundamental security properties in AI/ML systems (CIA triad)\n",
    "2. **Classify** attacks based on timing (training vs. test-time) and objectives\n",
    "3. **Analyze** threat models and adversarial capabilities\n",
    "4. **Formalize** security objectives for ML systems\n",
    "5. **Apply** attack taxonomy to real-world scenarios\n",
    "6. **Design** appropriate defenses based on threat analysis\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction to AI Security](#intro)\n",
    "2. [The CIA Triad in Machine Learning](#cia)\n",
    "3. [Attack Taxonomy Framework](#taxonomy)\n",
    "4. [Threat Modeling Methodology](#threat-modeling)\n",
    "5. [Attack Surface Analysis](#attack-surface)\n",
    "6. [Real-World Case Studies](#case-studies)\n",
    "7. [Defense Strategy Framework](#defense)\n",
    "8. [Exercises](#exercises)\n",
    "9. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636c7db",
   "metadata": {},
   "source": [
    "## **1. Introduction to AI Security** <a name=\"intro\"></a>\n",
    "\n",
    "### **Why is AI Security Different?**\n",
    "\n",
    "Traditional cybersecurity focuses on protecting systems from unauthorized access and code execution. **AI Security** adds unique challenges:\n",
    "\n",
    "| Traditional Security | AI Security |\n",
    "|---------------------|-------------|\n",
    "| Binary outcomes (works/fails) | Probabilistic outputs |\n",
    "| Code-based vulnerabilities | Data-based vulnerabilities |\n",
    "| Known attack patterns | Adaptive adversaries |\n",
    "| Deterministic behavior | Statistical learning |\n",
    "| Code inspection possible | Model internals opaque |\n",
    "\n",
    "### **The Machine Learning Pipeline Attack Surface**\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ   Data      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Training ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Model  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Deployment ‚îÇ\n",
    "‚îÇ Collection  ‚îÇ     ‚îÇ Process  ‚îÇ     ‚îÇ         ‚îÇ     ‚îÇ & Inference‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "      ‚ñ≤                  ‚ñ≤                ‚ñ≤                 ‚ñ≤\n",
    "      ‚îÇ                  ‚îÇ                ‚îÇ                 ‚îÇ\n",
    "   Poisoning         Backdoors        Model             Evasion\n",
    "   Attacks           Trojans          Theft             Attacks\n",
    "```\n",
    "\n",
    "Each stage presents unique attack opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add31633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from IPython.display import Image, display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869fc1d",
   "metadata": {},
   "source": [
    "## **2. The CIA Triad in Machine Learning** <a name=\"cia\"></a>\n",
    "\n",
    "The classic **CIA Triad** applies to ML systems with specific interpretations:\n",
    "\n",
    "### **2.1. Confidentiality**\n",
    "**Definition:** Protecting sensitive information from unauthorized access.\n",
    "\n",
    "**In ML Context:**\n",
    "- **Training data privacy:** Preventing leakage of training samples\n",
    "- **Model privacy:** Protecting model parameters and architecture\n",
    "- **Inference privacy:** Securing user queries and predictions\n",
    "\n",
    "**Example Attacks:**\n",
    "- Model Inversion: Reconstruct training data from model\n",
    "- Membership Inference: Determine if data was in training set\n",
    "- Model Extraction: Steal model functionality via queries\n",
    "\n",
    "### **2.2. Integrity**\n",
    "**Definition:** Ensuring data and model behavior are not maliciously altered.\n",
    "\n",
    "**In ML Context:**\n",
    "- **Data integrity:** Training data is not poisoned\n",
    "- **Model integrity:** Model behaves as intended\n",
    "- **Prediction integrity:** Outputs are trustworthy\n",
    "\n",
    "**Example Attacks:**\n",
    "- Data Poisoning: Inject malicious samples into training data\n",
    "- Backdoor Attacks: Embed hidden triggers in model\n",
    "- Model Poisoning: Corrupt model parameters or architecture\n",
    "\n",
    "### **2.3. Availability**\n",
    "**Definition:** Ensuring the system remains accessible and functional.\n",
    "\n",
    "**In ML Context:**\n",
    "- **Service availability:** Model can respond to queries\n",
    "- **Performance availability:** Acceptable inference latency\n",
    "- **Resource availability:** Computational resources not exhausted\n",
    "\n",
    "**Example Attacks:**\n",
    "- Sponge Attacks: Force high computational cost at inference\n",
    "- Denial of Service: Overwhelm model with queries\n",
    "- Resource Exhaustion: Deplete GPU/CPU resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9222a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# OPTIONAL: Check if widgets work in your environment\n",
    "# If not, you can fill this python dictionary manually for the plot.\n",
    "\n",
    "print(\"EXERCISE 1: CLASSIFY THE ATTACKS\")\n",
    "print(\"Map each attack to the correctly violated CIA principle.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "attacks_to_classify = [\n",
    "    'Model Inversion', 'Data Poisoning', 'Sponge Attack',\n",
    "    'Membership Inference', 'Backdoor Attack', 'Model Extraction',\n",
    "    'Denial of Service', 'Model Corruption'\n",
    "]\n",
    "\n",
    "# Ground truth for validation\n",
    "ground_truth = {\n",
    "    'Model Inversion': 'Confidentiality',\n",
    "    'Membership Inference': 'Confidentiality',\n",
    "    'Model Extraction': 'Confidentiality',\n",
    "    'Data Poisoning': 'Integrity',\n",
    "    'Backdoor Attack': 'Integrity',\n",
    "    'Model Corruption': 'Integrity',\n",
    "    'Sponge Attack': 'Availability',\n",
    "    'Denial of Service': 'Availability'\n",
    "}\n",
    "\n",
    "# Create Widgets\n",
    "dropdowns = {}\n",
    "style = {'description_width': 'initial'}\n",
    "for attack in attacks_to_classify:\n",
    "    dropdowns[attack] = widgets.Dropdown(\n",
    "        options=['Select...', 'Confidentiality', 'Integrity', 'Availability'],\n",
    "        description=f\"**{attack}**:\",\n",
    "        style=style,\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "\n",
    "check_button = widgets.Button(\n",
    "    description=\"Check Answers & Visualize\", \n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    layout=widgets.Layout(width='50%', margin='20px 0px 0px 0px'),\n",
    "    icon='check'\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "def check_and_plot(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        correct_count = 0\n",
    "        user_cia_counts = {'Confidentiality': 0, 'Integrity': 0, 'Availability': 0}\n",
    "        \n",
    "        all_selected = True\n",
    "        for attack, widget in dropdowns.items():\n",
    "            user_val = widget.value\n",
    "            if user_val == 'Select...':\n",
    "                print(f\"Please classify: {attack}\")\n",
    "                all_selected = False\n",
    "                continue # Keep checking others\n",
    "            \n",
    "            user_cia_counts[user_val] += 1\n",
    "            \n",
    "            if user_val == ground_truth[attack]:\n",
    "                correct_count += 1\n",
    "            else:\n",
    "                print(f\"{attack} is actually a violation of {ground_truth[attack]}\")\n",
    "\n",
    "        if not all_selected:\n",
    "            return\n",
    "\n",
    "        if correct_count == len(attacks_to_classify):\n",
    "            print(f\"\\nAll {correct_count} correct! Generating your Threat Landscape...\")\n",
    "            \n",
    "            # Plotting\n",
    "            fig, ax = plt.subplots(figsize=(10, 5))\n",
    "            principles = list(user_cia_counts.keys())\n",
    "            counts = list(user_cia_counts.values())\n",
    "            colors = ['#e74c3c', '#3498db', '#2ecc71'] # Red, Blue, Green\n",
    "            \n",
    "            bars = ax.bar(principles, counts, color=colors, alpha=0.7)\n",
    "            ax.set_title('CIA Violation Distribution (Based on your classification)', fontsize=14)\n",
    "            ax.set_ylabel('Number of Attacks')\n",
    "            ax.set_ylim(0, max(counts)+1)\n",
    "            \n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.annotate(f'{height}',\n",
    "                            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                            xytext=(0, 3),  # 3 points vertical offset\n",
    "                            textcoords=\"offset points\",\n",
    "                            ha='center', va='bottom', fontweight='bold')\n",
    "            \n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"\\nScore: {correct_count}/{len(attacks_to_classify)}. Please correct the errors above and try again.\")\n",
    "\n",
    "check_button.on_click(check_and_plot)\n",
    "\n",
    "# Layout\n",
    "ui = widgets.VBox(list(dropdowns.values()) + [check_button])\n",
    "display(ui, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5aab0",
   "metadata": {},
   "source": [
    "## **3. Attack Taxonomy Framework** <a name=\"taxonomy\"></a>\n",
    "\n",
    "We classify attacks along multiple dimensions:\n",
    "\n",
    "### **Dimension 1: Attack Timing**\n",
    "\n",
    "#### **Training-Time Attacks (Poisoning)**\n",
    "- Occur during model training\n",
    "- Attacker manipulates training data or process\n",
    "- Effects persist in deployed model\n",
    "- Examples: Data poisoning, backdoor injection\n",
    "\n",
    "#### **Test-Time Attacks (Evasion)**\n",
    "- Occur during inference/deployment\n",
    "- Attacker manipulates input to deployed model\n",
    "- No modification to model itself\n",
    "- Examples: Adversarial examples, adversarial prompts\n",
    "\n",
    "### **Dimension 2: Security Objective Violated**\n",
    "\n",
    "| Attack Category | CIA Principle | Timing | Goal |\n",
    "|----------------|---------------|--------|------|\n",
    "| **Evasion** | Integrity | Test-time | Misclassify specific inputs |\n",
    "| **Poisoning** | Integrity | Training-time | Corrupt model behavior |\n",
    "| **Privacy** | Confidentiality | Any | Extract sensitive info |\n",
    "| **Sponge** | Availability | Test-time | Degrade performance |\n",
    "| **Model Extraction** | Confidentiality | Test-time | Steal model functionality |\n",
    "\n",
    "### **Dimension 3: Attacker Knowledge**\n",
    "\n",
    "#### **White-Box Attacks**\n",
    "- Full knowledge of model architecture\n",
    "- Access to model parameters\n",
    "- Can compute gradients\n",
    "- Most powerful attacks\n",
    "\n",
    "#### **Black-Box Attacks**\n",
    "- No access to model internals\n",
    "- Only query-response access\n",
    "- Must infer model behavior\n",
    "- More realistic threat model\n",
    "\n",
    "#### **Gray-Box Attacks**\n",
    "- Partial knowledge (e.g., architecture but not weights)\n",
    "- Limited access to internals\n",
    "- Between white-box and black-box\n",
    "\n",
    "### **Dimension 4: Attack Specificity**\n",
    "\n",
    "#### **Targeted Attacks**\n",
    "- Goal: Cause specific misclassification\n",
    "- Example: Make \"stop sign\" classified as \"speed limit\"\n",
    "- Harder to achieve\n",
    "- More dangerous in practice\n",
    "\n",
    "#### **Untargeted Attacks**\n",
    "- Goal: Cause any misclassification\n",
    "- Example: Make model fail on any input\n",
    "- Easier to achieve\n",
    "- Useful for measuring robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d633124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE 2: BUILD THE ATTACK TAXONOMY\n",
    "# Replace the '?' question marks with the correct values based on the lecture content.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# TODO: Complete the dictionary lists below\n",
    "# Timing Options: 'Test-time' or 'Training-time'\n",
    "# CIA Options: 'Integrity', 'Confidentiality', or 'Availability'\n",
    "\n",
    "student_taxonomy_data = {\n",
    "    'Attack Type': ['FGSM', 'Data Poisoning', 'Model Inversion', 'Sponge Attack'],\n",
    "    'Timing': ['?', '?', '?', '?'], \n",
    "    'CIA Violation': ['?', '?', '?', '?'] \n",
    "}\n",
    "\n",
    "# --- STUDENT CODE START ---\n",
    "# student_taxonomy_data['Timing'] = ['Test-time', 'Training-time', 'Test-time', 'Test-time'] # Example\n",
    "# --- STUDENT CODE END ---\n",
    "\n",
    "def verify_taxonomy(data):\n",
    "    try:\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Ground Truth\n",
    "        correct_timing = ['Test-time', 'Training-time', 'Test-time', 'Test-time']\n",
    "        correct_cia = ['Integrity', 'Integrity', 'Confidentiality', 'Availability']\n",
    "        \n",
    "        score = 0\n",
    "        if '?' in data['Timing'] or '?' in data['CIA Violation']:\n",
    "             print(\"‚ö†Ô∏è Please replace all '?' with valid values.\")\n",
    "             return\n",
    "\n",
    "        if list(df['Timing']) == correct_timing:\n",
    "            print(\"‚úÖ Timing column is correct.\")\n",
    "            score += 1\n",
    "        else:\n",
    "            print(f\"‚ùå Timing column has errors. Expected similar to: {correct_timing}\")\n",
    "            \n",
    "        if list(df['CIA Violation']) == correct_cia:\n",
    "            print(\"‚úÖ CIA Violation column is correct.\")\n",
    "            score += 1\n",
    "        else:\n",
    "            print(f\"‚ùå CIA Violation column has errors.\")\n",
    "            \n",
    "        if score == 2:\n",
    "            print(\"\\nüèÜ Excellent! Taxonomy Table Constructed:\")\n",
    "            display(df)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Uncomment to check your work:\n",
    "# verify_taxonomy(student_taxonomy_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7718b",
   "metadata": {},
   "source": [
    "## **4. Threat Modeling Methodology** <a name=\"threat-modeling\"></a>\n",
    "\n",
    "### **STRIDE Framework for ML**\n",
    "\n",
    "Adapted from Microsoft's STRIDE model:\n",
    "\n",
    "| Threat | ML Interpretation | Example |\n",
    "|--------|------------------|----------|\n",
    "| **S**poofing | Impersonate legitimate data/user | Fake training samples |\n",
    "| **T**ampering | Modify data or model | Poisoning attacks |\n",
    "| **R**epudiation | Deny actions | Untraceable adversarial examples |\n",
    "| **I**nformation Disclosure | Leak sensitive data | Model inversion |\n",
    "| **D**enial of Service | Make system unavailable | Sponge attacks |\n",
    "| **E**levation of Privilege | Gain unauthorized capabilities | Jailbreak LLMs |\n",
    "\n",
    "---\n",
    "\n",
    "### **Threat Modeling Process**\n",
    "\n",
    "**Step 1: Define System Boundaries**\n",
    "```\n",
    "What components are in scope?\n",
    "- Data collection pipeline\n",
    "- Training infrastructure\n",
    "- Deployed model\n",
    "- User interface\n",
    "```\n",
    "\n",
    "**Step 2: Identify Assets**\n",
    "```\n",
    "What needs protection?\n",
    "- Training data (privacy)\n",
    "- Model parameters (IP)\n",
    "- Model predictions (integrity)\n",
    "- System availability\n",
    "```\n",
    "\n",
    "**Step 3: Characterize Adversary**\n",
    "```\n",
    "What can the attacker do?\n",
    "- Access level (white/gray/black-box)\n",
    "- Resources (compute, data, expertise)\n",
    "- Motivation (financial, sabotage, espionage)\n",
    "```\n",
    "\n",
    "**Step 4: Enumerate Attack Paths**\n",
    "```\n",
    "How can attacks be executed?\n",
    "- Training data poisoning\n",
    "- Test-time evasion\n",
    "- Model extraction\n",
    "- Privacy attacks\n",
    "```\n",
    "\n",
    "**Step 5: Prioritize Risks**\n",
    "```\n",
    "Which threats are most critical?\n",
    "Risk = Likelihood √ó Impact\n",
    "```\n",
    "\n",
    "**Step 6: Design Mitigations**\n",
    "```\n",
    "How to defend?\n",
    "- Input validation\n",
    "- Adversarial training\n",
    "- Differential privacy\n",
    "- Monitoring & detection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9f834",
   "metadata": {},
   "source": [
    "## **4.1 Exercise: Interactive STRIDE Threat Modeling**\n",
    "\n",
    "**Scenario**: You are securing a **Medical Imaging AI System** that diagnoses diseases from X-rays. The system interacts with a Hospital Database (PACS) and provides a web interface for doctors.\n",
    "\n",
    "**Task**: Use the `ThreatModel` class below to identify at least one threat for each STRIDE category in this scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreatModel:\n",
    "    def __init__(self, system_name):\n",
    "        self.system_name = system_name\n",
    "        self.threats = {\n",
    "            'Spoofing': [],\n",
    "            'Tampering': [],\n",
    "            'Repudiation': [],\n",
    "            'Information Disclosure': [],\n",
    "            'Denial of Service': [],\n",
    "            'Elevation of Privilege': []\n",
    "        }\n",
    "    \n",
    "    def add_threat(self, category, description):\n",
    "        \"\"\"Adds a threat to a specific STRIDE category.\"\"\"\n",
    "        if category in self.threats:\n",
    "            self.threats[category].append(description)\n",
    "            print(f\"Confirmed: Added '{description}' to [{category}]\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error: '{category}' is not a valid STRIDE category.\")\n",
    "\n",
    "    def analyze_coverage(self):\n",
    "        \"\"\"Visualizes the threat coverage.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        categories = list(self.threats.keys())\n",
    "        counts = [len(self.threats[k]) for k in categories]\n",
    "        \n",
    "        # Check gaps\n",
    "        gaps = [k for k, v in self.threats.items() if len(v) == 0]\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        bars = plt.bar(categories, counts, color='#9b59b6')\n",
    "        plt.title(f'Threat Coverage for {self.system_name}')\n",
    "        plt.ylabel('Number of Threats Identified')\n",
    "        plt.xlabel('STRIDE Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, height, f'{int(height)}', ha='center', va='bottom')\n",
    "            \n",
    "        plt.show()\n",
    "        \n",
    "        if not gaps:\n",
    "            print(\"‚úÖ Great job! You have addressed all STRIDE categories.\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: You are missing threats for: {', '.join(gaps)}\")\n",
    "\n",
    "# Initialize the threat model\n",
    "med_system_threats = ThreatModel(\"Medical X-Ray AI\")\n",
    "\n",
    "# --- TODO: ADD YOUR THREATS BELOW ---\n",
    "# Think: How could an attacker Spoof a doctor? Tamper with an X-ray? \n",
    "# med_system_threats.add_threat('Spoofing', 'Attacker uses stolen doctor credentials')\n",
    "# med_system_threats.add_threat('Tampering', '...')\n",
    "\n",
    "\n",
    "# --- Run Analysis ---\n",
    "# med_system_threats.analyze_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33583133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Assessment Matrix\n",
    "class ThreatModel:\n",
    "    def __init__(self, system_name):\n",
    "        self.system_name = system_name\n",
    "        self.threats = []\n",
    "    \n",
    "    def add_threat(self, name, likelihood, impact, category):\n",
    "        \"\"\"Add a threat to the model.\n",
    "        \n",
    "        Args:\n",
    "            likelihood: 1-5 (1=rare, 5=almost certain)\n",
    "            impact: 1-5 (1=negligible, 5=catastrophic)\n",
    "        \"\"\"\n",
    "        risk_score = likelihood * impact\n",
    "        self.threats.append({\n",
    "            'name': name,\n",
    "            'likelihood': likelihood,\n",
    "            'impact': impact,\n",
    "            'risk': risk_score,\n",
    "            'category': category\n",
    "        })\n",
    "    \n",
    "    def visualize_risk_matrix(self):\n",
    "        \"\"\"Create risk assessment visualization.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Risk Matrix\n",
    "        df = pd.DataFrame(self.threats)\n",
    "        \n",
    "        # Scatter plot\n",
    "        scatter = ax1.scatter(df['likelihood'], df['impact'], \n",
    "                            s=df['risk']*30, alpha=0.6, c=df['risk'], \n",
    "                            cmap='RdYlGn_r', edgecolors='black', linewidth=1.5)\n",
    "        \n",
    "        # Add labels\n",
    "        for idx, row in df.iterrows():\n",
    "            ax1.annotate(row['name'], (row['likelihood'], row['impact']),\n",
    "                        fontsize=8, ha='center')\n",
    "        \n",
    "        ax1.set_xlabel('Likelihood', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Impact', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title(f'Risk Matrix: {self.system_name}', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlim([0, 6])\n",
    "        ax1.set_ylim([0, 6])\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add risk zones\n",
    "        ax1.axhline(y=3, color='orange', linestyle='--', alpha=0.5)\n",
    "        ax1.axvline(x=3, color='orange', linestyle='--', alpha=0.5)\n",
    "        ax1.text(1.5, 5.5, 'Low\\nLikelihood\\nHigh Impact', ha='center', fontsize=9)\n",
    "        ax1.text(4.5, 5.5, 'CRITICAL\\nZONE', ha='center', fontsize=11, \n",
    "                fontweight='bold', color='red')\n",
    "        \n",
    "        plt.colorbar(scatter, ax=ax1, label='Risk Score')\n",
    "        \n",
    "        # Bar chart of risks\n",
    "        df_sorted = df.sort_values('risk', ascending=True)\n",
    "        colors = ['green' if r < 10 else 'orange' if r < 15 else 'red' \n",
    "                 for r in df_sorted['risk']]\n",
    "        \n",
    "        ax2.barh(df_sorted['name'], df_sorted['risk'], color=colors, alpha=0.7)\n",
    "        ax2.set_xlabel('Risk Score', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Threat Prioritization', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('threat_model_risk_matrix.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return df.sort_values('risk', ascending=False)\n",
    "\n",
    "# Example: Medical Diagnosis AI System\n",
    "medical_ai = ThreatModel(\"Medical Diagnosis AI System\")\n",
    "\n",
    "# Add threats\n",
    "medical_ai.add_threat('Data Poisoning', likelihood=3, impact=5, category='Integrity')\n",
    "medical_ai.add_threat('Model Inversion', likelihood=4, impact=5, category='Privacy')\n",
    "medical_ai.add_threat('Adversarial Examples', likelihood=4, impact=4, category='Integrity')\n",
    "medical_ai.add_threat('Sponge Attack', likelihood=2, impact=3, category='Availability')\n",
    "medical_ai.add_threat('Model Extraction', likelihood=3, impact=3, category='IP Theft')\n",
    "medical_ai.add_threat('Membership Inference', likelihood=4, impact=4, category='Privacy')\n",
    "\n",
    "# Visualize\n",
    "risk_summary = medical_ai.visualize_risk_matrix()\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"THREAT PRIORITIZATION (Highest Risk First)\")\n",
    "print(\"=\"*70)\n",
    "print(risk_summary[['name', 'likelihood', 'impact', 'risk', 'category']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906aeef4",
   "metadata": {},
   "source": [
    "## **5. Attack Surface Analysis** <a name=\"attack-surface\"></a>\n",
    "\n",
    "### **ML System Attack Surface Map**\n",
    "\n",
    "Every component in the ML pipeline presents attack opportunities:\n",
    "\n",
    "#### **1. Data Collection & Preparation**\n",
    "**Attack Vectors:**\n",
    "- Inject malicious samples\n",
    "- Corrupt labels\n",
    "- Manipulate feature distributions\n",
    "- Introduce biases\n",
    "\n",
    "**Defenses:**\n",
    "- Data validation\n",
    "- Outlier detection\n",
    "- Statistical testing\n",
    "- Provenance tracking\n",
    "\n",
    "#### **2. Model Training**\n",
    "**Attack Vectors:**\n",
    "- Backdoor insertion\n",
    "- Hyperparameter manipulation\n",
    "- Training process sabotage\n",
    "- Loss function tampering\n",
    "\n",
    "**Defenses:**\n",
    "- Secure training environments\n",
    "- Gradient inspection\n",
    "- Checkpoint verification\n",
    "- Adversarial training\n",
    "\n",
    "#### **3. Model Deployment**\n",
    "**Attack Vectors:**\n",
    "- Model substitution\n",
    "- API exploitation\n",
    "- Query-based attacks\n",
    "- Timing attacks\n",
    "\n",
    "**Defenses:**\n",
    "- Model signing\n",
    "- Rate limiting\n",
    "- Input sanitization\n",
    "- Anomaly detection\n",
    "\n",
    "#### **4. Inference & Serving**\n",
    "**Attack Vectors:**\n",
    "- Adversarial inputs\n",
    "- Privacy extraction\n",
    "- Resource exhaustion\n",
    "- Side-channel attacks\n",
    "\n",
    "**Defenses:**\n",
    "- Input validation\n",
    "- Output filtering\n",
    "- Differential privacy\n",
    "- Resource monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c14d9b",
   "metadata": {},
   "source": [
    "## **5.1 Exercise: Attack Tree Generation**\n",
    "\n",
    "**Goal**: Model Evasion (forcing the AI to misclassify a tumor as healthy tissue).\n",
    "\n",
    "**Task**: Complete the attack tree by adding leaf nodes (specific attack vectors) for the Physical and Digital domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_attack_tree(tree_dict, root, level=0):\n",
    "    indent = \"    \" * level\n",
    "    icon = \"üå≥\" if level == 0 else (\"‚îú‚îÄ‚îÄ\" if level < 2 else \"‚îî‚îÄ‚îÄ\")\n",
    "    print(f\"{indent}{icon} {root}\")\n",
    "    \n",
    "    if root in tree_dict:\n",
    "        for child in tree_dict[root]:\n",
    "            render_attack_tree(tree_dict, child, level + 1)\n",
    "\n",
    "# The Root Goal\n",
    "root_node = \"Evasion: False Negative Diagnosis\"\n",
    "\n",
    "# TODO: Fill in the attack vectors below\n",
    "attack_tree_structure = {\n",
    "    root_node: [\n",
    "        \"Physical Domain (Real-world)\",\n",
    "        \"Digital Domain (Pixel-space)\"\n",
    "    ],\n",
    "    \"Physical Domain (Real-world)\": [\n",
    "        # Example: 'Place adversarial sticker on patient chest'\n",
    "        \"TODO: Add physical vector 1\",\n",
    "        \"TODO: Add physical vector 2\"\n",
    "    ],\n",
    "    \"Digital Domain (Pixel-space)\": [\n",
    "        # Example: 'Apply PGD noise to DICOM image'\n",
    "        \"TODO: Add digital vector 1\",\n",
    "        \"TODO: Add digital vector 2\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# --- STUDENT CODE: Update the dictionary above ---\n",
    "\n",
    "# Visualize\n",
    "print(\"ATTACK TREE VISUALIZATION:\")\n",
    "print(\"=\"*40)\n",
    "render_attack_tree(attack_tree_structure, root_node)\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04c8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack Surface Visualization\n",
    "def plot_attack_surface():\n",
    "    \"\"\"Visualize attack surface across ML pipeline stages.\"\"\"\n",
    "    \n",
    "    stages = ['Data\\nCollection', 'Feature\\nEngineering', 'Model\\nTraining', \n",
    "              'Validation', 'Deployment', 'Inference']\n",
    "    \n",
    "    # Attack surface score (0-10) for each stage\n",
    "    attack_surface = [8, 6, 9, 4, 7, 10]\n",
    "    detectability = [6, 5, 4, 7, 5, 3]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    x = np.arange(len(stages))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, attack_surface, width, label='Attack Surface Size',\n",
    "                   color='#e74c3c', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, detectability, width, label='Attack Detectability',\n",
    "                   color='#3498db', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('ML Pipeline Stage', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score (0-10)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Attack Surface Analysis Across ML Pipeline', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stages)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim([0, 11])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('attack_surface_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(\"  ‚Ä¢ Inference stage has HIGHEST attack surface (score: 10)\")\n",
    "    print(\"  ‚Ä¢ Training stage attacks are HARDEST to detect (score: 4)\")\n",
    "    print(\"  ‚Ä¢ Validation provides BEST detection opportunity (score: 7)\")\n",
    "    print(\"  ‚Ä¢ Multi-stage defense strategy is essential\\n\")\n",
    "\n",
    "plot_attack_surface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d14e40",
   "metadata": {},
   "source": [
    "## **6. Real-World Case Studies** <a name=\"case-studies\"></a>\n",
    "\n",
    "### **Case Study 1: Autonomous Vehicle Adversarial Attack**\n",
    "\n",
    "**Scenario:** Stop sign misclassification attack\n",
    "\n",
    "**Attack Details:**\n",
    "- Physical adversarial patches on stop signs\n",
    "- Classifier misidentifies as \"Speed Limit 45\"\n",
    "- Attack succeeds at multiple angles/distances\n",
    "\n",
    "**Threat Model:**\n",
    "- **Attacker Access:** Black-box (no model access)\n",
    "- **Attack Type:** Physical evasion attack\n",
    "- **CIA Violation:** Integrity\n",
    "- **Impact:** Safety-critical failure\n",
    "\n",
    "**Lessons:**\n",
    "1. Physical-world attacks are feasible\n",
    "2. Black-box attacks can be effective\n",
    "3. Safety-critical systems need robust defenses\n",
    "\n",
    "---\n",
    "\n",
    "### **Case Study 2: Microsoft Tay Chatbot Poisoning**\n",
    "\n",
    "**Scenario:** Online learning chatbot corrupted via user interactions\n",
    "\n",
    "**Attack Details:**\n",
    "- Users repeatedly fed offensive content\n",
    "- Model learned and reproduced toxic behavior\n",
    "- Bot taken offline within 24 hours\n",
    "\n",
    "**Threat Model:**\n",
    "- **Attacker Access:** Data poisoning via normal interface\n",
    "- **Attack Type:** Training-time poisoning\n",
    "- **CIA Violation:** Integrity\n",
    "- **Impact:** Reputational damage, service shutdown\n",
    "\n",
    "**Lessons:**\n",
    "1. Online learning is highly vulnerable\n",
    "2. User-generated data requires validation\n",
    "3. Content filtering is essential\n",
    "4. Human oversight needed for public-facing AI\n",
    "\n",
    "---\n",
    "\n",
    "### **Case Study 3: Netflix Prize Privacy Breach**\n",
    "\n",
    "**Scenario:** Re-identification of users from anonymized ratings\n",
    "\n",
    "**Attack Details:**\n",
    "- Researchers cross-referenced Netflix data with IMDb\n",
    "- Successfully identified users from \"anonymous\" dataset\n",
    "- Revealed sensitive viewing preferences\n",
    "\n",
    "**Threat Model:**\n",
    "- **Attacker Access:** Public dataset\n",
    "- **Attack Type:** Privacy attack via linkage\n",
    "- **CIA Violation:** Confidentiality\n",
    "- **Impact:** Privacy violation, lawsuit\n",
    "\n",
    "**Lessons:**\n",
    "1. Anonymization alone is insufficient\n",
    "2. Auxiliary information enables re-identification\n",
    "3. Differential privacy needed for public release\n",
    "4. Privacy risks in seemingly safe data sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize case studies\n",
    "case_studies = pd.DataFrame({\n",
    "    'Case Study': [\n",
    "        'Stop Sign Attack',\n",
    "        'Tay Chatbot',\n",
    "        'Netflix Prize'\n",
    "    ],\n",
    "    'Domain': [\n",
    "        'Autonomous Vehicles',\n",
    "        'Conversational AI',\n",
    "        'Recommender Systems'\n",
    "    ],\n",
    "    'Attack Type': [\n",
    "        'Physical Evasion',\n",
    "        'Data Poisoning',\n",
    "        'Privacy Linkage'\n",
    "    ],\n",
    "    'CIA Violated': [\n",
    "        'Integrity',\n",
    "        'Integrity',\n",
    "        'Confidentiality'\n",
    "    ],\n",
    "    'Impact': [\n",
    "        'Safety-Critical',\n",
    "        'Reputational',\n",
    "        'Privacy Breach'\n",
    "    ],\n",
    "    'Year': [2017, 2016, 2007],\n",
    "    'Key Lesson': [\n",
    "        'Physical attacks feasible',\n",
    "        'Online learning vulnerable',\n",
    "        'Anonymization insufficient'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"REAL-WORLD AI SECURITY INCIDENTS\")\n",
    "print(\"=\"*100 + \"\\n\")\n",
    "print(case_studies.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Timeline visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "years = case_studies['Year'].values\n",
    "names = case_studies['Case Study'].values\n",
    "colors_map = {'Integrity': '#e74c3c', 'Confidentiality': '#3498db'}\n",
    "colors = [colors_map[x] for x in case_studies['CIA Violated']]\n",
    "\n",
    "ax.scatter(years, [1]*len(years), s=500, c=colors, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for year, name, y_offset in zip(years, names, [0.15, -0.15, 0.15]):\n",
    "    ax.annotate(name, (year, 1), (year, 1 + y_offset),\n",
    "               fontsize=10, ha='center', fontweight='bold',\n",
    "               arrowprops=dict(arrowstyle='->', lw=1.5))\n",
    "\n",
    "ax.set_xlabel('Year', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Timeline of Notable AI Security Incidents', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim([0.5, 1.5])\n",
    "ax.set_yticks([])\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16feb887",
   "metadata": {},
   "source": [
    "## **7. Defense Strategy Framework** <a name=\"defense\"></a>\n",
    "\n",
    "### **Defense-in-Depth for ML Systems**\n",
    "\n",
    "No single defense is sufficient. Layer multiple protections:\n",
    "\n",
    "#### **Layer 1: Data Protection**\n",
    "- Input validation and sanitization\n",
    "- Outlier detection\n",
    "- Data provenance tracking\n",
    "- Adversarial example detection\n",
    "\n",
    "#### **Layer 2: Model Hardening**\n",
    "- Adversarial training\n",
    "- Certified defenses\n",
    "- Regularization techniques\n",
    "- Ensemble methods\n",
    "\n",
    "#### **Layer 3: Privacy Protection**\n",
    "- Differential privacy\n",
    "- Federated learning\n",
    "- Homomorphic encryption\n",
    "- Secure multi-party computation\n",
    "\n",
    "#### **Layer 4: Monitoring & Detection**\n",
    "- Anomaly detection\n",
    "- Behavioral analysis\n",
    "- Performance monitoring\n",
    "- Audit logging\n",
    "\n",
    "#### **Layer 5: Incident Response**\n",
    "- Model rollback capabilities\n",
    "- Attack mitigation procedures\n",
    "- Forensic analysis\n",
    "- Recovery protocols\n",
    "\n",
    "---\n",
    "\n",
    "### **Defense Selection Matrix**\n",
    "\n",
    "| Attack Type | Primary Defense | Secondary Defense | Detection Method |\n",
    "|-------------|----------------|-------------------|------------------|\n",
    "| Evasion | Adversarial Training | Input Validation | Anomaly Detection |\n",
    "| Data Poisoning | Outlier Removal | RONI Testing | Statistical Testing |\n",
    "| Backdoor | Neural Cleanse | Fine-tuning | Trigger Detection |\n",
    "| Model Inversion | Differential Privacy | Output Perturbation | Query Monitoring |\n",
    "| Sponge Attack | Inference Timeout | Input Filtering | Resource Monitoring |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baddfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defense-in-Depth Visualization\n",
    "def visualize_defense_layers():\n",
    "    \"\"\"Create layered defense visualization.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Define concentric circles for defense layers\n",
    "    layers = [\n",
    "        {'radius': 5, 'label': 'Data Protection', 'color': '#3498db'},\n",
    "        {'radius': 4, 'label': 'Model Hardening', 'color': '#2ecc71'},\n",
    "        {'radius': 3, 'label': 'Privacy Protection', 'color': '#f39c12'},\n",
    "        {'radius': 2, 'label': 'Monitoring', 'color': '#e74c3c'},\n",
    "        {'radius': 1, 'label': 'Core Model', 'color': '#9b59b6'}\n",
    "    ]\n",
    "    \n",
    "    for layer in layers:\n",
    "        circle = plt.Circle((0, 0), layer['radius'], \n",
    "                           color=layer['color'], alpha=0.3, \n",
    "                           linewidth=3, edgecolor='black')\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # Add label\n",
    "        angle = np.pi / 4\n",
    "        x = layer['radius'] * 0.7 * np.cos(angle)\n",
    "        y = layer['radius'] * 0.7 * np.sin(angle)\n",
    "        ax.text(x, y, layer['label'], fontsize=11, fontweight='bold',\n",
    "               ha='center', va='center',\n",
    "               bbox=dict(boxstyle='round', facecolor='white', edgecolor='black'))\n",
    "    \n",
    "    ax.set_xlim([-6, 6])\n",
    "    ax.set_ylim([-6, 6])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Defense-in-Depth for ML Systems', fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Add attack arrows\n",
    "    attack_angles = [0, np.pi/2, np.pi, 3*np.pi/2]\n",
    "    attack_names = ['Evasion', 'Poisoning', 'Privacy', 'Sponge']\n",
    "    \n",
    "    for angle, name in zip(attack_angles, attack_names):\n",
    "        start_x = 5.5 * np.cos(angle)\n",
    "        start_y = 5.5 * np.sin(angle)\n",
    "        end_x = 0.8 * np.cos(angle)\n",
    "        end_y = 0.8 * np.sin(angle)\n",
    "        \n",
    "        ax.annotate('', xy=(end_x, end_y), xytext=(start_x, start_y),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "        ax.text(start_x * 1.15, start_y * 1.15, name, fontsize=10,\n",
    "               ha='center', va='center', color='red', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('defense_in_depth.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "visualize_defense_layers()\n",
    "print(\"\\nDefense-in-Depth visualization created!\")\n",
    "print(\"\\nKey Principle: Multiple layers provide redundancy\")\n",
    "print(\"   If one defense fails, others still protect the system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be431db",
   "metadata": {},
   "source": [
    "## **8. Exercises** <a name=\"exercises\"></a>\n",
    "\n",
    "### **Exercise 1: Threat Modeling Practice (Medium)**\n",
    "\n",
    "Choose a real-world ML system:\n",
    "- Facial recognition system\n",
    "- Credit card fraud detection\n",
    "- Spam email filter\n",
    "- Content recommendation engine\n",
    "\n",
    "Create a complete threat model:\n",
    "1. Define system boundaries and assets\n",
    "2. Identify 5-7 potential threats\n",
    "3. Assess likelihood and impact for each\n",
    "4. Create a risk matrix\n",
    "5. Propose defense strategies\n",
    "\n",
    "**Deliverable:** Use the `ThreatModel` class to document your analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 2: Attack Classification (Easy)**\n",
    "\n",
    "For each scenario, classify the attack:\n",
    "\n",
    "**Scenario A:** An attacker adds imperceptible noise to images to fool an image classifier.\n",
    "- Timing: ?\n",
    "- CIA: ?\n",
    "- Access: ?\n",
    "\n",
    "**Scenario B:** A malicious insider corrupts 5% of training labels in a dataset.\n",
    "- Timing: ?\n",
    "- CIA: ?\n",
    "- Access: ?\n",
    "\n",
    "**Scenario C:** An attacker queries a model repeatedly to reconstruct its decision boundary.\n",
    "- Timing: ?\n",
    "- CIA: ?\n",
    "- Access: ?\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 3: Defense Design (Hard)**\n",
    "\n",
    "Design a multi-layered defense strategy for a medical diagnosis AI system that:\n",
    "1. Processes patient data (sensitive)\n",
    "2. Provides treatment recommendations\n",
    "3. Must be highly accurate and trustworthy\n",
    "4. Faces threats from multiple adversaries\n",
    "\n",
    "Requirements:\n",
    "- Address all three CIA properties\n",
    "- Include 3+ defense layers\n",
    "- Specify detection mechanisms\n",
    "- Consider regulatory compliance (HIPAA)\n",
    "\n",
    "---\n",
    "\n",
    "### **Exercise 4: Case Study Analysis (Medium)**\n",
    "\n",
    "Research the **ClearView AI privacy controversy**:\n",
    "1. What attack/vulnerability was exploited?\n",
    "2. Which CIA principle was violated?\n",
    "3. What was the impact?\n",
    "4. How could it have been prevented?\n",
    "5. What defenses would you recommend?\n",
    "\n",
    "**Format:** 2-page analysis with threat model and defense recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb45088",
   "metadata": {},
   "source": [
    "## **9. Conclusion** <a name=\"conclusion\"></a>\n",
    "\n",
    "### **What You Learned**\n",
    "\n",
    "- **CIA Triad:** How confidentiality, integrity, and availability apply to ML  \n",
    "- **Attack Taxonomy:** Classification by timing, objective, and access  \n",
    "- **Threat Modeling:** Systematic analysis of adversarial risks  \n",
    "- **Attack Surface:** Vulnerability points across ML pipeline  \n",
    "- **Defense Strategies:** Multi-layered protection approaches  \n",
    "- **Real-World Context:** Case studies from actual incidents  \n",
    "\n",
    "### **Key Principles**\n",
    "\n",
    "1. **No Perfect Defense:** Security is about risk management, not elimination\n",
    "2. **Context Matters:** Threat models vary by application domain\n",
    "3. **Defense-in-Depth:** Multiple layers provide resilience\n",
    "4. **Trade-offs:** Security often conflicts with accuracy/performance\n",
    "5. **Evolving Threats:** Continuous monitoring and adaptation required\n",
    "\n",
    "### **Preparing for Upcoming Labs**\n",
    "\n",
    "Now that you understand threat modeling, you're ready to:\n",
    "\n",
    "**Module 2:** Implement and defend against evasion attacks  \n",
    "**Module 3-4:** Execute and detect poisoning attacks  \n",
    "**Module 5:** Create and mitigate sponge attacks  \n",
    "**Module 6:** Launch and prevent privacy attacks  \n",
    "**Module 7:** Generate and evaluate synthetic data  \n",
    "**Module 8:** Deploy comprehensive defense systems  \n",
    "\n",
    "Each subsequent lab will reference this threat modeling framework.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Resources**\n",
    "\n",
    "**Foundational Papers:**\n",
    "- [Adversarial Examples Are Not Bugs, They Are Features (Ilyas et al., 2019)](https://arxiv.org/abs/1905.02175)\n",
    "- [SoK: Security and Privacy in Machine Learning (Papernot et al., 2018)](https://ieeexplore.ieee.org/document/8406613)\n",
    "- [The NIST Adversarial ML Framework](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2023.pdf)\n",
    "\n",
    "**Industry Standards:**\n",
    "- MITRE ATLAS: Adversarial Threat Landscape for AI Systems\n",
    "- OWASP Machine Learning Security Top 10\n",
    "- ISO/IEC 24029: AI Trustworthiness\n",
    "\n",
    "**Tools & Frameworks:**\n",
    "- [Microsoft Threat Modeling Tool](https://www.microsoft.com/en-us/securityengineering/sdl/threatmodeling)\n",
    "- [Adversarial Robustness Toolbox (ART)](https://github.com/Trusted-AI/adversarial-robustness-toolbox)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privaudit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
